{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_ap():\n",
    "    # Datos extra√≠dos del reporte\n",
    "    metrics = ['mAP@0.5', 'AP@0.5 Car', 'AP@0.5 Pedestrian']\n",
    "    ap_values = [0.791, 0.8089, 0.5588]\n",
    "    colors = ['orange', 'blue', 'lightgreen']\n",
    "    \n",
    "    x = np.arange(len(metrics))  # Posiciones en el eje X\n",
    "    \n",
    "    plt.figure(figsize=(9, 4))\n",
    "    \n",
    "    # Dibujar barras\n",
    "    bars = plt.bar(x, ap_values, color=colors, alpha=0.3, width=0.6)\n",
    "    \n",
    "    # A√±adir valores encima de cada barra\n",
    "    for bar, value in zip(bars, ap_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.01, f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Etiquetas y formato\n",
    "    plt.ylabel('Value', fontsize=14)\n",
    "    plt.title('mAP@0.5 and AP@0.5 by class', fontsize=16)\n",
    "    plt.xticks(x, metrics, fontsize=11, rotation=15)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Llamada a la funci√≥n\n",
    "plot_ap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_evaluation_file(file_path):\n",
    "    \"\"\"Parses the evaluation file and extracts mAP@0.5 for each class in each video.\"\"\"\n",
    "    class_map = defaultdict(list)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    current_class = None\n",
    "    \n",
    "    for line in lines:\n",
    "        video_match = re.match(r'=== Video: (\\d+) ===', line)\n",
    "        class_match = re.match(r'--- Class: (\\w+) \\(ID: \\d+\\) ---', line)\n",
    "        map_match = re.match(r'\\s*Average Precision.*IoU=0.50\\s+\\| area=\\s+all \\| maxDets=100 \\] = ([0-9\\.]+)', line)\n",
    "        \n",
    "        if class_match:\n",
    "            current_class = class_match.group(1)\n",
    "        elif map_match and current_class:\n",
    "            class_map[current_class].append(float(map_match.group(1)))\n",
    "    \n",
    "    return class_map\n",
    "\n",
    "def plot_histograms(class_map):\n",
    "    \"\"\"Plots a single histogram with different colors for each class based on mAP@0.5 values.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = ['blue', 'lightgreen']  # Define a set of colors\n",
    "    \n",
    "    for idx, (class_name, values) in enumerate(class_map.items()):\n",
    "        plt.hist(values, bins=4, alpha=0.3, color=colors[idx % len(colors)], edgecolor='black', label=class_name)\n",
    "    \n",
    "    plt.xlabel('AP@0.5', fontsize=16)\n",
    "    plt.ylabel('Frequency', fontsize=16)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.title('AP@0.5 by class and video', fontsize=18)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso\n",
    "file_path = \"/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/huggingface/evaluation_results/off-the-shelf/evaluation/per_class_metrics.txt\"  # Reempl√°zalo con la ruta de tu archivo\n",
    "class_map = parse_evaluation_file(file_path)\n",
    "plot_histograms(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def draw_boxes(image, annotations, color, label_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Dibuja cajas en la imagen seg√∫n las anotaciones proporcionadas.\n",
    "    \"\"\"\n",
    "    for ann in annotations:\n",
    "        x, y, w, h = ann['bbox']\n",
    "        category_id = ann['category_id']\n",
    "        score = ann.get('score', None)  # Solo predicciones tienen score\n",
    "\n",
    "        label = f\"{label_prefix}ID:{category_id}\"\n",
    "        if score is not None:\n",
    "            label += f\" {score:.2f}\"\n",
    "\n",
    "        # Dibujar rect√°ngulo\n",
    "        cv2.rectangle(image, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)\n",
    "\n",
    "        # Poner etiqueta\n",
    "        cv2.putText(image, label, (int(x), int(y - 5)), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.5, color, 2, cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "def create_video_from_coco(gt_file, pred_file, frames_folder, output_video, fps=30):\n",
    "    \"\"\"\n",
    "    Genera un video con las detecciones de Ground Truth y predicciones sobre los frames.\n",
    "    \"\"\"\n",
    "    # Cargar anotaciones COCO\n",
    "    with open(gt_file, 'r') as f:\n",
    "        gt_data = json.load(f)\n",
    "\n",
    "    with open(pred_file, 'r') as f:\n",
    "        pred_data = json.load(f)\n",
    "\n",
    "    # Crear diccionario de anotaciones por imagen\n",
    "    gt_annotations = {img[\"id\"]: [] for img in gt_data[\"images\"]}\n",
    "    pred_annotations = {img[\"id\"]: [] for img in gt_data[\"images\"]}\n",
    "\n",
    "    for ann in gt_data[\"annotations\"]:\n",
    "        gt_annotations[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    for ann in pred_data:\n",
    "        pred_annotations[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    # Obtener lista de im√°genes ordenadas por ID\n",
    "    images = sorted(gt_data[\"images\"], key=lambda x: x[\"id\"])\n",
    "    \n",
    "    # Obtener tama√±o del video desde el primer frame\n",
    "    first_frame = cv2.imread(os.path.join(frames_folder, '000000.png'))\n",
    "    height, width, _ = first_frame.shape\n",
    "\n",
    "    # Inicializar el escritor de video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    frames_list = os.listdir(frames_folder)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        frame_path = os.path.join(frames_folder, frames_list[i])\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is None:\n",
    "            continue  # Saltar si el frame no se encuentra\n",
    "\n",
    "        image_id = img[\"id\"]\n",
    "\n",
    "        # Dibujar Ground Truth en verde\n",
    "        frame = draw_boxes(frame, gt_annotations.get(image_id, []), (0, 255, 0), \"GT \")\n",
    "\n",
    "        # Dibujar predicciones en rojo\n",
    "        frame = draw_boxes(frame, pred_annotations.get(image_id, []), (0, 0, 255), \"Pred \")\n",
    "\n",
    "        # Escribir frame en el video\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"‚úÖ Video guardado en: {output_video}\")\n",
    "\n",
    "# üìå Ejemplo de uso:\n",
    "gt = '/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/huggingface/evaluation_results/off-the-shelf/ground_truth/gt_coco_0006.json'\n",
    "preds = '/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/huggingface/evaluation_results/off-the-shelf/predictions/preds_coco_0006.json'\n",
    "frames = '/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/KITTI_MOTS/training/image_02/0006'\n",
    "\n",
    "output_video = \"output.mp4\"\n",
    "\n",
    "create_video_from_coco(gt, preds, frames, output_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def class_name(class_id: int):\n",
    "    if class_id == 2:\n",
    "        return \"pedestrian\"\n",
    "    elif class_id == 1:\n",
    "        return \"car\"\n",
    "    \n",
    "def load_json_data(json_file):\n",
    "    \"\"\"Load JSON data from a file\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def process_frame_with_boxes(image_path, gt_data=None, pred_data=None, show_gt=True, show_pred=True):\n",
    "    \"\"\"\n",
    "    Process an image and overlay bounding boxes from ground truth and prediction data.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        gt_data (dict): Ground truth data in COCO format\n",
    "        pred_data (list): Prediction data as a list of annotation dictionaries\n",
    "        show_gt (bool): Whether to show ground truth boxes\n",
    "        show_pred (bool): Whether to show prediction boxes\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Processed image with bounding boxes\n",
    "    \"\"\"\n",
    "    # Check if image exists\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Failed to read image: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract image ID from filename (assuming format like 000123.png)\n",
    "    filename = os.path.basename(image_path)\n",
    "    frame_number = int(os.path.splitext(filename)[0])\n",
    "    \n",
    "    # Find image ID in ground truth data\n",
    "    image_id = None\n",
    "    if gt_data is not None:\n",
    "        for img_info in gt_data.get('images', []):\n",
    "            if frame_number == img_info.get('id') or filename == os.path.basename(img_info.get('file_name', '')):\n",
    "                image_id = img_info.get('id')\n",
    "                break\n",
    "    \n",
    "    if image_id is None:\n",
    "        return image  # No annotations for this image\n",
    "    \n",
    "    # Draw ground truth boxes if available\n",
    "    if show_gt and gt_data is not None:\n",
    "        for ann in gt_data.get('annotations', []):\n",
    "            if ann.get('image_id') == image_id:\n",
    "                bbox = ann.get('bbox', [])\n",
    "                category_id = ann.get('category_id', 0)\n",
    "                if len(bbox) == 4:  # Ensure bbox has correct format [x, y, width, height]\n",
    "                    x, y, w, h = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "                    # Draw green box for ground truth\n",
    "                    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "    # Draw prediction boxes if available\n",
    "    if show_pred and pred_data is not None:\n",
    "        for ann in pred_data:\n",
    "            if ann.get('image_id') == image_id:\n",
    "                bbox = ann.get('bbox', [])\n",
    "                category_id = ann.get('category_id', 0)\n",
    "                if len(bbox) == 4:  # Ensure bbox has correct format\n",
    "                    x, y, w, h = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "                    # Draw red box for predictions\n",
    "                    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "                    # Add label with score\n",
    "                    cv2.putText(image, f\"{class_name(category_id)}\", (x, y - 5), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def create_video_from_frames(input_dir, output_file, gt_data=None, pred_data=None, show_gt=True, show_pred=True, fps=20):\n",
    "    \"\"\"\n",
    "    Create a video from frames with bounding boxes for detections\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Directory containing image frames\n",
    "        output_file (str): Path to save the output video\n",
    "        gt_data (dict): Ground truth data in COCO format\n",
    "        pred_data (list): Prediction data as a list of annotation dictionaries\n",
    "        show_gt (bool): Whether to show ground truth boxes\n",
    "        show_pred (bool): Whether to show prediction boxes\n",
    "        fps (int): Frames per second for output video\n",
    "    \"\"\"\n",
    "    # Get all frame files sorted\n",
    "    frame_files = sorted([f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg'))])\n",
    "    \n",
    "    if not frame_files:\n",
    "        print(f\"No image files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Get first frame to determine video dimensions\n",
    "    first_frame_path = os.path.join(input_dir, frame_files[0])\n",
    "    first_frame = cv2.imread(first_frame_path)\n",
    "    if first_frame is None:\n",
    "        print(f\"Failed to read first frame: {first_frame_path}\")\n",
    "        return\n",
    "    \n",
    "    height, width, _ = first_frame.shape\n",
    "    \n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'XVID'\n",
    "    video_writer = cv2.VideoWriter(output_file, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Creating video from {len(frame_files)} frames...\")\n",
    "    \n",
    "    # Process each frame and add to video\n",
    "    for frame_file in tqdm(frame_files):\n",
    "        frame_path = os.path.join(input_dir, frame_file)\n",
    "        processed_frame = process_frame_with_boxes(\n",
    "            frame_path, gt_data, pred_data, show_gt, show_pred\n",
    "        )\n",
    "        \n",
    "        if processed_frame is not None:\n",
    "            video_writer.write(processed_frame)\n",
    "    \n",
    "    # Release resources\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {output_file}\")\n",
    "\n",
    "\n",
    "video = '0006'\n",
    "# Example usage\n",
    "gt_file = f\"/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/huggingface/evaluation_results/off-the-shelf/ground_truth/gt_coco_{video}.json\"\n",
    "gt_data = load_json_data(gt_file)\n",
    "\n",
    "pred_file = f'/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/huggingface/evaluation_results/off-the-shelf/predictions/preds_coco_{video}.json'\n",
    "pred_data = load_json_data(pred_file)\n",
    "\n",
    "# Path to image frames directory\n",
    "image_dir = f\"/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/KITTI_MOTS/training/image_02/{video}\"\n",
    "\n",
    "# Path for output video\n",
    "output_video = \"/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/huggingface/output_video_with_both_detections.mp4\"\n",
    "\n",
    "# Create video with both ground truth and prediction bounding boxes\n",
    "create_video_from_frames(\n",
    "    input_dir=image_dir,\n",
    "    output_file=output_video,\n",
    "    gt_data=gt_data,\n",
    "    pred_data=pred_data,  # Including prediction data\n",
    "    show_gt=True,\n",
    "    show_pred=True,  # Show predictions\n",
    "    fps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def video_to_gif(video_path, output_path=\"output.gif\", duration=100, loop=0):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Fin del video\n",
    "\n",
    "        # Convertir a RGB (cv2 usa BGR por defecto)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "        frames.append(Image.fromarray(frame))\n",
    "\n",
    "    cap.release()  # Liberar el video\n",
    "\n",
    "    if frames:\n",
    "        frames[0].save(output_path, save_all=True, append_images=frames[0:], duration=duration, loop=loop)\n",
    "        print(f\"GIF saved at {output_path}\")\n",
    "    else:\n",
    "        print(\"Error: No frames extracted from video.\")\n",
    "\n",
    "path_video = '/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/huggingface/pedestrians_mal.mp4'\n",
    "video_to_gif(path_video, \"output.gif\", duration=75, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_json_data(json_file):\n",
    "    \"\"\"Load JSON data from a file\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def draw_label(img, text, x, y, color=(0, 0, 255), font_scale=0.3, thickness=1):\n",
    "    \"\"\"\n",
    "    Draws a text label with a semi-transparent background.\n",
    "    \n",
    "    Args:\n",
    "        img: Image on which to draw\n",
    "        text: Text to display\n",
    "        x, y: Position of the text\n",
    "        color: Text color\n",
    "        font_scale: Font size\n",
    "        thickness: Thickness of the text\n",
    "    \"\"\"\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "    text_w, text_h = text_size[0], text_size[1]\n",
    "\n",
    "    # Background rectangle\n",
    "    bg_x1, bg_y1 = x, y - text_h - 5\n",
    "    bg_x2, bg_y2 = x + text_w + 10, y\n",
    "    overlay = img.copy()\n",
    "    cv2.rectangle(overlay, (bg_x1, bg_y1), (bg_x2, bg_y2), color, -1)\n",
    "\n",
    "    # Blend with transparency\n",
    "    alpha = 0.4  # Transparency factor\n",
    "    cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, img)\n",
    "\n",
    "    # Draw text\n",
    "    cv2.putText(img, text, (x + 5, y - 5), font, font_scale, (255, 255, 255), thickness, cv2.LINE_AA)\n",
    "\n",
    "def process_frame_with_boxes(image_path, gt_data=None, pred_data=None, show_gt=True, show_pred=True, category_map={}):\n",
    "    \"\"\"\n",
    "    Process an image and overlay bounding boxes from ground truth and prediction data with a modern look.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        gt_data (dict): Ground truth data in COCO format\n",
    "        pred_data (list): Prediction data as a list of annotation dictionaries\n",
    "        show_gt (bool): Whether to show ground truth boxes\n",
    "        show_pred (bool): Whether to show prediction boxes\n",
    "        category_map (dict): Mapping of category IDs to category names\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Processed image with bounding boxes\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Failed to read image: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    filename = os.path.basename(image_path)\n",
    "    frame_number = int(os.path.splitext(filename)[0])\n",
    "\n",
    "    image_id = None\n",
    "    if gt_data is not None:\n",
    "        for img_info in gt_data.get('images', []):\n",
    "            if frame_number == img_info.get('id') or filename == os.path.basename(img_info.get('file_name', '')):\n",
    "                image_id = img_info.get('id')\n",
    "                break\n",
    "\n",
    "    if image_id is None:\n",
    "        return image  # No annotations for this image\n",
    "\n",
    "    # Draw ground truth boxes (only rectangles, no text)\n",
    "    if show_gt and gt_data is not None:\n",
    "        for ann in gt_data.get('annotations', []):\n",
    "            if ann.get('image_id') == image_id:\n",
    "                bbox = ann.get('bbox', [])\n",
    "                if len(bbox) == 4:\n",
    "                    x, y, w, h = map(int, bbox)\n",
    "                    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Draw prediction boxes (with class name, but no confidence score)\n",
    "    if show_pred and pred_data is not None:\n",
    "        for ann in pred_data:\n",
    "            if ann.get('image_id') == image_id:\n",
    "                bbox = ann.get('bbox', [])\n",
    "                category_id = ann.get('category_id', 0)\n",
    "                class_name = category_map.get(category_id, f\"ID {category_id}\")\n",
    "\n",
    "                if len(bbox) == 4:\n",
    "                    x, y, w, h = map(int, bbox)\n",
    "                    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "                    draw_label(image, class_name, x, y)\n",
    "\n",
    "    return image\n",
    "\n",
    "def create_video_from_frames(input_dir, output_file, gt_data=None, pred_data=None, show_gt=True, show_pred=True, fps=20, category_map={}):\n",
    "    \"\"\"\n",
    "    Create a video from frames with bounding boxes for detections with improved visualization.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Directory containing image frames\n",
    "        output_file (str): Path to save the output video\n",
    "        gt_data (dict): Ground truth data in COCO format\n",
    "        pred_data (list): Prediction data as a list of annotation dictionaries\n",
    "        show_gt (bool): Whether to show ground truth boxes\n",
    "        show_pred (bool): Whether to show prediction boxes\n",
    "        fps (int): Frames per second for output video\n",
    "        category_map (dict): Mapping of category IDs to category names\n",
    "    \"\"\"\n",
    "    frame_files = sorted([f for f in os.listdir(input_dir) if f.endswith(('.png', '.jpg'))])\n",
    "\n",
    "    if not frame_files:\n",
    "        print(f\"No image files found in {input_dir}\")\n",
    "        return\n",
    "\n",
    "    first_frame_path = os.path.join(input_dir, frame_files[0])\n",
    "    first_frame = cv2.imread(first_frame_path)\n",
    "    if first_frame is None:\n",
    "        print(f\"Failed to read first frame: {first_frame_path}\")\n",
    "        return\n",
    "\n",
    "    height, width, _ = first_frame.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_file, fourcc, fps, (width, height))\n",
    "\n",
    "    print(f\"Creating video from {len(frame_files)} frames...\")\n",
    "\n",
    "    for frame_file in tqdm(frame_files):\n",
    "        frame_path = os.path.join(input_dir, frame_file)\n",
    "        processed_frame = process_frame_with_boxes(\n",
    "            frame_path, gt_data, pred_data, show_gt, show_pred, category_map\n",
    "        )\n",
    "\n",
    "        if processed_frame is not None:\n",
    "            video_writer.write(processed_frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {output_file}\")\n",
    "\n",
    "video = '0017'\n",
    "# Example usage\n",
    "gt_file = f\"/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/huggingface/evaluation_results/off-the-shelf/ground_truth/gt_coco_{video}.json\"\n",
    "gt_data = load_json_data(gt_file)\n",
    "\n",
    "pred_file = f'/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/huggingface/evaluation_results/off-the-shelf/predictions/preds_coco_{video}.json'\n",
    "pred_data = load_json_data(pred_file)\n",
    "\n",
    "# Path to image frames directory\n",
    "image_dir = f\"/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/KITTI_MOTS/training/image_02/{video}\"\n",
    "\n",
    "# Path for output video\n",
    "output_video = \"/Users/arnaubarrera/Desktop/MSc Computer Vision/C5. Visual Recognition/mcv-c5-group-3/huggingface/output_video_with_both_detections.mp4\"\n",
    "\n",
    "\n",
    "# Example category map (replace with your actual categories)\n",
    "category_map = {\n",
    "    1: \"car\",\n",
    "    2: \"pedestrian\",\n",
    "}\n",
    "\n",
    "create_video_from_frames(\n",
    "    input_dir=image_dir,\n",
    "    output_file=output_video,\n",
    "    gt_data=gt_data,\n",
    "    pred_data=pred_data,\n",
    "    show_gt=True,\n",
    "    show_pred=True,\n",
    "    fps=10,\n",
    "    category_map=category_map\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
