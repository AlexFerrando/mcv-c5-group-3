{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 - Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from typing import Tuple\n",
    "from torchtune.modules.tokenizers._utils import BaseTokenizer\n",
    "import torch\n",
    "\n",
    "class FoodDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: str,\n",
    "            tokenizer: BaseTokenizer,\n",
    "            transform: torch.nn.Sequential,\n",
    "        ):\n",
    "        data_path = Path(data_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Define df\n",
    "        self.df = pd.read_csv(data_path / 'Food Ingredients and Recipe Dataset with Image Name Mapping.csv')\n",
    "        # Keep only the title and image name\n",
    "        self.df = self.df[['Title', 'Image_Name']]\n",
    "        # Remove rows with invalid 'Image_Name' entries (e.g., '#NAME?')\n",
    "        self.df = self.df[self.df['Image_Name'] != '#NAME?']\n",
    "        # Remove nans\n",
    "        self.df = self.df.dropna() # There are 5 nans xd\n",
    "\n",
    "        # Define image_path\n",
    "        self.images_folder = data_path / 'Food Images/Food Images'\n",
    "        \n",
    "        print(f'Loaded {len(self.df)} samples')\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        row = self.df.iloc[idx]\n",
    "        title = row['Title']\n",
    "        # img_name we have to add .jpg\n",
    "        img_name = row['Image_Name'] + '.jpg'\n",
    "        img_path = self.images_folder / img_name\n",
    "        \n",
    "        return self.process_image(img_path), self.process_text(title)\n",
    "\n",
    "    def process_image(self, img_path: Path) -> torch.Tensor:\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(image)\n",
    "    \n",
    "    def process_text(self, text: str) -> torch.Tensor:\n",
    "        return torch.Tensor(self.tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Character Tokenizer\n",
    "Splits text into individual **characters**, including letters, punctuation, and spaces.\n",
    "\n",
    "Input:  \"Hello!\" --> Tokens: ['H', 'e', 'l', 'l', 'o', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "class CharacterTokenizer:\n",
    "    def __init__(self, dataset_titles: List[str] = None, text_max_len: int = 201):\n",
    "        self.sos_token = '<SOS>'\n",
    "        self.eos_token = '<EOS>'\n",
    "        self.pad_token = '<PAD>'\n",
    "\n",
    "        # Original character list\n",
    "        self.chars = [\n",
    "            self.sos_token, self.eos_token, self.pad_token, '\\n', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '+', ',',\n",
    "            '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '9', ':', ';', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H',\n",
    "            'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd',\n",
    "            'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "            '\\x92', '\\x96', '\\xa0', '®', 'Á', 'É', 'à', 'á', 'â', 'ã', 'ä', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï',\n",
    "            'ñ', 'ò', 'ó', 'ô', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ō', 'ơ', '̀', '́', '̃', '̉', 'С', 'и', 'к', 'н', 'р', 'ы',\n",
    "            '\\u2009', '–', '—', '‘', '’', '“', '”', '강', '개', '닭', '된', '장', '전', '정', '찌', '파'\n",
    "        ]\n",
    "\n",
    "        # Optionally add new characters from dataset\n",
    "        if dataset_titles:\n",
    "            extra_chars = set(''.join(dataset_titles)) - set(self.chars)\n",
    "            self.chars += sorted(extra_chars)\n",
    "\n",
    "        self.idx2char = {k: v for k, v in enumerate(self.chars)}\n",
    "        self.char2idx = {v: k for k, v in enumerate(self.chars)}\n",
    "        self.text_max_len = text_max_len\n",
    "\n",
    "    def encode(self, text: str, **kwargs: Dict[str, Any]) -> List[int]:\n",
    "        encoded = [self.char2idx[self.sos_token]] + [\n",
    "            self.char2idx.get(char, self.char2idx[self.pad_token]) for char in text\n",
    "        ] + [self.char2idx[self.eos_token]]\n",
    "        encoded += [self.char2idx[self.pad_token]] * (self.text_max_len - len(encoded))\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, tokens: List[int], **kwargs: Dict[str, Any]) -> str:\n",
    "        tokens = [\n",
    "            token for token in tokens\n",
    "            if token not in [\n",
    "                self.char2idx[self.sos_token],\n",
    "                self.char2idx[self.eos_token],\n",
    "                self.char2idx[self.pad_token]\n",
    "            ]\n",
    "        ]\n",
    "        return ''.join([self.idx2char.get(token, '') for token in tokens])\n",
    "\n",
    "    def batch_decode(self, batch_tokens: List[List[int]], **kwargs: Dict[str, Any]) -> List[str]:\n",
    "        return [self.decode(tokens) for tokens in batch_tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Word Tokenizer\n",
    "Splits text into **full words**, typically by whitespace and punctuation.\n",
    "\n",
    "Input: \"Hello world!\" --> Tokens: ['Hello', 'world']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class WordTokenizer:\n",
    "    def __init__(self, texts: List[str] = None, text_max_len: int = 50):\n",
    "        self.sos_token = \"<SOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        self.text_max_len = text_max_len\n",
    "\n",
    "        self.vocab = [self.sos_token, self.eos_token, self.pad_token, self.unk_token]\n",
    "\n",
    "        if texts:\n",
    "            self.build_vocab(texts)\n",
    "        else:\n",
    "            self.word2idx = {}\n",
    "            self.idx2word = {}\n",
    "\n",
    "    def build_vocab(self, texts: List[str]):\n",
    "        words = set()\n",
    "        for text in texts:\n",
    "            tokens = text.split()\n",
    "            words.update(tokens)\n",
    "        self.vocab += sorted(words)\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        tokens = [self.sos_token] + text.split()[:self.text_max_len - 2] + [self.eos_token]\n",
    "        token_ids = [\n",
    "            self.word2idx.get(token, self.word2idx[self.unk_token]) for token in tokens\n",
    "        ]\n",
    "        token_ids += [self.word2idx[self.pad_token]] * (self.text_max_len - len(token_ids))\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        return \" \".join([\n",
    "            self.idx2word.get(token, self.unk_token)\n",
    "            for token in tokens\n",
    "            if token not in {\n",
    "                self.word2idx[self.sos_token],\n",
    "                self.word2idx[self.eos_token],\n",
    "                self.word2idx[self.pad_token]\n",
    "            }\n",
    "        ])\n",
    "\n",
    "    def batch_decode(self, batch_tokens: List[List[int]]) -> List[str]:\n",
    "        return [self.decode(tokens) for tokens in batch_tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. WordPiece tokenizer (BERT)\n",
    "Slits words into **subword units**, using a learned vocabulary of common pieces. Used in models like BERT and RoBERTa.\n",
    "\n",
    "Input: \"Unbelievable\" --> Tokens: ['un', '##bel', '##iev', '##able']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from typing import List\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, pretrained_model_name=\"bert-base-cased\", text_max_len=50):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.text_max_len = text_max_len\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        return self.tokenizer.encode(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.text_max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        return self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "    def batch_decode(self, batch_tokens: List[List[int]]) -> List[str]:\n",
    "        return self.tokenizer.batch_decode(batch_tokens, skip_special_tokens=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tokenizer.vocab_size\n",
    "    \n",
    "def clean_decoded_text(text: str) -> str:\n",
    "    text = text.replace(\" - \", \"-\")        \n",
    "    text = text.replace(\" ’ \", \"’\")         \n",
    "    text = \" \".join(text.split())           \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13466 samples\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import v2\n",
    "from torch import nn\n",
    "\n",
    "DATA_PATH = '/Users/Usuario/Documents/MCV/C5/week3/archive'\n",
    "\n",
    "tokenizer = CharacterTokenizer()\n",
    "\n",
    "transform = nn.Sequential(\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize((224, 224), antialias=True),\n",
    "    v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    ")\n",
    "    \n",
    "\n",
    "dataset = FoodDataset(data_path=DATA_PATH, tokenizer=tokenizer, transform=transform)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: Grilled Fennel-Rubbed Triple-Cut Pork Chops\n",
      "Char Encoded: [0, 35, 72, 63, 66, 66, 59, 58, 4, 34, 59, 68, 68, 59, 66, 15, 46, 75, 56, 56, 59, 58, 4, 48, 72, 63, 70, 66, 59, 15, 31, 75, 74, 4, 44, 69, 72, 65, 4, 31, 62, 69, 70, 73, 1, 2, 2, 2, 2, 2]\n",
      "Char Decoded: Grilled Fennel-Rubbed Triple-Cut Pork Chops\n",
      "Word Encoded: [0, 3326, 2808, 7160, 5440, 1864, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Word Decoded: Grilled Fennel-Rubbed Triple-Cut Pork Chops\n",
      "WordPiece Encoded: [101, 144, 26327, 1181, 27868, 8967, 118, 155, 10354, 4774, 9457, 118, 15411, 18959, 102]\n",
      "WordPiece Decoded: Grilled Fennel-Rubbed Triple-Cut Po\n",
      "\n",
      "Original Text: Cumin-Scented Stir-Fried Beef with Celery\n",
      "Char Encoded: [0, 31, 75, 67, 63, 68, 15, 47, 57, 59, 68, 74, 59, 58, 4, 47, 74, 63, 72, 15, 34, 72, 63, 59, 58, 4, 30, 59, 59, 60, 4, 77, 63, 74, 62, 4, 31, 59, 66, 59, 72, 79, 1, 2, 2, 2, 2, 2, 2, 2]\n",
      "Char Decoded: Cumin-Scented Stir-Fried Beef with Celery\n",
      "Word Encoded: [0, 2350, 6645, 857, 7680, 1572, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Word Decoded: Cumin-Scented Stir-Fried Beef with Celery\n",
      "WordPiece Encoded: [101, 140, 14088, 1179, 118, 20452, 22666, 1457, 3161, 118, 13359, 4830, 16385, 2087, 102]\n",
      "WordPiece Decoded: Cumin-Scented Stir-Fried Beef\n",
      "\n",
      "Original Text: Country Hash\n",
      "Char Encoded: [0, 31, 69, 75, 68, 74, 72, 79, 4, 36, 55, 73, 62, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Char Decoded: Country Hash\n",
      "Word Encoded: [0, 2176, 3426, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Word Decoded: Country Hash\n",
      "WordPiece Encoded: [101, 3898, 10736, 1324, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "WordPiece Decoded: Country Hash\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Pull sample titles from the dataset\n",
    "sample_texts = [dataset.df.iloc[i][\"Title\"] for i in random.sample(range(len(dataset)), 3)]\n",
    "\n",
    "# Extract all titles for vocab building\n",
    "all_titles = dataset.df[\"Title\"].dropna().tolist()\n",
    "\n",
    "# Initialize tokenizers with full dataset\n",
    "char_tokenizer = CharacterTokenizer(dataset_titles=all_titles, text_max_len=50)\n",
    "word_tokenizer = WordTokenizer(texts=all_titles, text_max_len=15)\n",
    "wordpiece_tokenizer = WordPieceTokenizer(pretrained_model_name=\"bert-base-cased\", text_max_len=15)\n",
    "\n",
    "for text in sample_texts:\n",
    "    print(f\"\\nOriginal Text: {text}\")\n",
    "    \n",
    "    # Character-level\n",
    "    encoded_char = char_tokenizer.encode(text)\n",
    "    print(f\"Char Encoded: {encoded_char}\")\n",
    "    print(f\"Char Decoded: {char_tokenizer.decode(encoded_char)}\")\n",
    "    \n",
    "    # Word-level\n",
    "    encoded_word = word_tokenizer.encode(text)\n",
    "    print(f\"Word Encoded: {encoded_word}\")\n",
    "    print(f\"Word Decoded: {word_tokenizer.decode(encoded_word)}\")\n",
    "    \n",
    "    # WordPiece-level\n",
    "    encoded_wp = wordpiece_tokenizer.encode(text)\n",
    "    raw_decoded_wp = wordpiece_tokenizer.decode(encoded_wp)\n",
    "    cleaned_wp = clean_decoded_text(raw_decoded_wp)\n",
    "    print(f\"WordPiece Encoded: {encoded_wp}\")\n",
    "    print(f\"WordPiece Decoded: {cleaned_wp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from typing import List\n",
    "import nltk\n",
    "\n",
    "class Metric():\n",
    "    def __init__(self):\n",
    "        self._download_nltk_resources()\n",
    "        self.bleu = evaluate.load('bleu')\n",
    "        self.rouge = evaluate.load('rouge')\n",
    "        self.meteor = evaluate.load('meteor')\n",
    "        \n",
    "    def _download_nltk_resources(self):\n",
    "        \"\"\"Download NLTK resources quietly\"\"\"\n",
    "        try:\n",
    "            nltk.data.find('wordnet')\n",
    "        except LookupError:\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "            \n",
    "        try:\n",
    "            nltk.data.find('punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            \n",
    "        try:\n",
    "            nltk.data.find('omw-1.4')\n",
    "        except LookupError:\n",
    "            nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "    def compute_metrics(self, ground_truth: List[str], prediction: List[str]):\n",
    "        res_b = self.bleu.compute(predictions=prediction, references=ground_truth)\n",
    "        res_r = self.rouge.compute(predictions=prediction, references=ground_truth)\n",
    "        res_m = self.meteor.compute(predictions=prediction, references=ground_truth)\n",
    "\n",
    "        return {'bleu': res_b, 'rouge': res_r, 'meteor': res_m}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check largest titles to define text_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest Title (112 chars):\n",
      "Hummus-Crusted Alaskan Wild King Salmon Over a Bed of French Beans, Red Onion, and Cucumber Salad with Lemon Oil\n",
      "\n",
      "Longest Title (19 words):\n",
      "Hummus-Crusted Alaskan Wild King Salmon Over a Bed of French Beans, Red Onion, and Cucumber Salad with Lemon Oil\n"
     ]
    }
   ],
   "source": [
    "# Longest title (by character count)\n",
    "longest_title = max(dataset.df['Title'].dropna(), key=len)\n",
    "print(f\"Longest Title ({len(longest_title)} chars):\\n{longest_title}\")\n",
    "\n",
    "# Longest title (by number of words)\n",
    "longest_by_words = max(dataset.df['Title'].dropna(), key=lambda x: len(x.split()))\n",
    "print(f\"\\nLongest Title ({len(longest_by_words.split())} words):\\n{longest_by_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Character Tokenizer Metrics:\n",
      "{'bleu': {'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 161, 'reference_length': 161}, 'rouge': {'rouge1': 1.0, 'rouge2': 0.9666666666666667, 'rougeL': 1.0, 'rougeLsum': 1.0}, 'meteor': {'meteor': 0.9684720182678573}}\n",
      "\n",
      "Word Tokenizer Metrics:\n",
      "{'bleu': {'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 161, 'reference_length': 161}, 'rouge': {'rouge1': 1.0, 'rouge2': 0.9666666666666667, 'rougeL': 1.0, 'rougeLsum': 1.0}, 'meteor': {'meteor': 0.9684720182678573}}\n",
      "\n",
      "WordPiece Tokenizer Metrics:\n",
      "{'bleu': {'bleu': 0.9656954349207257, 'precisions': [0.9815950920245399, 0.9699248120300752, 0.9615384615384616, 0.95], 'brevity_penalty': 1.0, 'length_ratio': 1.0124223602484472, 'translation_length': 163, 'reference_length': 161}, 'rouge': {'rouge1': 1.0, 'rouge2': 0.9666666666666667, 'rougeL': 1.0, 'rougeLsum': 1.0}, 'meteor': {'meteor': 0.9468663284096891}}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from metrics import Metric \n",
    "\n",
    "sample_texts = [dataset.df.iloc[i][\"Title\"] for i in random.sample(range(len(dataset)), 30)]\n",
    "all_titles = dataset.df[\"Title\"].dropna().tolist()\n",
    "\n",
    "char_tokenizer = CharacterTokenizer(dataset_titles=all_titles, text_max_len=130)\n",
    "word_tokenizer = WordTokenizer(texts=all_titles, text_max_len=25)\n",
    "wordpiece_tokenizer = WordPieceTokenizer(pretrained_model_name=\"bert-base-cased\", text_max_len=40)\n",
    "\n",
    "metric = Metric()\n",
    "\n",
    "gt = []     \n",
    "char_preds = []\n",
    "word_preds = []\n",
    "wp_preds = []\n",
    "\n",
    "for text in sample_texts:\n",
    "    gt.append([text]) \n",
    "\n",
    "    # Char\n",
    "    char_out = char_tokenizer.decode(char_tokenizer.encode(text))\n",
    "    char_preds.append(char_out)\n",
    "\n",
    "    # Word\n",
    "    word_out = word_tokenizer.decode(word_tokenizer.encode(text))\n",
    "    word_preds.append(word_out)\n",
    "\n",
    "    # WordPiece\n",
    "    wp_raw = wordpiece_tokenizer.decode(wordpiece_tokenizer.encode(text))\n",
    "    wp_clean = clean_decoded_text(wp_raw)\n",
    "    wp_preds.append(wp_clean)\n",
    "\n",
    "# -- Compute metrics --\n",
    "print(\"\\nCharacter Tokenizer Metrics:\")\n",
    "print(metric.compute_metrics(gt, char_preds))\n",
    "\n",
    "print(\"\\nWord Tokenizer Metrics:\")\n",
    "print(metric.compute_metrics(gt, word_preds))\n",
    "\n",
    "print(\"\\nWordPiece Tokenizer Metrics:\")\n",
    "print(metric.compute_metrics(gt, wp_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c1_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
