# Week 3: Image Captioning (I)

âš ï¸âš ï¸âš ï¸ Check our final slides [here](https://docs.google.com/presentation/d/1rZsFNczXgs0ZNEDZWPDb83qw1RwR42B14yrA6DDos6A/edit#slide=id.g340d9e9dd17_0_361)!

ğŸ“ğŸ“ğŸ“ See the paper for the project [here](https://overleaf.cvc.uab.es/project/67dff51c85f1f209c7a4c396)

Welcome to **Week 3** of our project, where we focus on **Image Captioning**. This repository contains all the necessary scripts and modules to train and evaluate captioning models for food images.
We will train and evaluate our models in the [Food Ingredients and Recipes Dataset with Images](https://www.kaggle.com/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images). Objective will be to be able to generate the title from a food image. An example can be found here:

### ğŸ½ï¸ Example:

<img src="https://assets.epicurious.com/photos/5f99a91e819b886aba0a2846/1:1/w_1920,c_limit/Chickensgiving_HERO_RECIPE_101920_1374_VOG_final.jpg" alt="Example Food Image" width="400"/>

**Desired Caption:** *"Miso-Butter Roast Chicken With Acorn Squash Panzanella"*

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ tokenizers/                 # Experiments with different tokenizers
â”œâ”€â”€ Baseline Model and Metrics.ipynb # Notebook with baseline experiments and metric evaluation
â”œâ”€â”€ consts.py                   # Constants used throughout the project
â”œâ”€â”€ dataset.py                  # Dataset class to read the FoodDataset
â”œâ”€â”€ inference.py                 # Generate captions for images using trained models
â”œâ”€â”€ metrics.py                   # Metric implementations for evaluation (e.g., BLEU, METEOR, CIDEr)
â”œâ”€â”€ models.py                    # Implementation of different captioning models
â”œâ”€â”€ our_tokenizers.py            # Custom tokenizer implementations
â”œâ”€â”€ sweep_arnau.py               # Hyperparameter tuning script
â”œâ”€â”€ test.ipynb                   # Testing notebook for inference & visualization
â”œâ”€â”€ train.py                     # Main training script
â””â”€â”€ tokenizers updated/          # Additional tokenizer experiments
```

---

## ğŸ‹ï¸â€â™‚ï¸ Training the Model

The main training script is **`train.py`**, which handles the training process for different captioning models. To train a model, simply run:

```bash
python train.py
```

Ensure you have all dependencies installed before running the script.

---

## ğŸ“– Dataset

The dataset used in this project is **FoodDataset**. The `dataset.py` file contains the `FoodDataset` class, which is responsible for loading and preprocessing the data.

ğŸ”— **Dataset link:** (https://www.kaggle.com/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images)

---

## ğŸ—ï¸ Models Implemented

The `models.py` file contains different architectures used in this project:

- **BaselineModel** â€“ A simple baseline model.
- **LSTMModel** â€“ A model using LSTMs for sequence generation.
- **LSTMWithAttention** â€“ An advanced model incorporating attention mechanisms.

---

## ğŸ–¼ï¸ Running Inference

To generate captions for an image using a trained model, modify the paths in `inference.py` and run it modifiying the paths in the code for the one desired.

```bash
python inference.py
```

This will generate a caption based on the trained model.

---

## ğŸ”¤ Tokenizers

Inside the `tokenizers/` folder, we have isolated experiments for different tokenizers. The implemented tokenizers in `our_tokenizers.py` include:

- **Character-level tokenizer**
- **BERT Tokenizer (WordPiece-level)**
- **Word-level tokenizer**

These tokenizers can be used to preprocess captions before training.

---

## ğŸš€ Installation & Requirements

Make sure you have all required dependencies installed:

```bash
pip install -r requirements.txt
```

Then, youâ€™re all set to train and test your image captioning models! ğŸ‰

---

## ğŸ“Œ Notes
- Ensure your dataset is properly downloaded and linked.
- Modify configurations in `consts.py` for hyperparameter tuning.
- Experiment with different tokenizers to improve performance.

For any questions, feel free to reach out! ğŸš€

