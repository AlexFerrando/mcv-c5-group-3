# Week 4: Image Captioning (II)

âš ï¸âš ï¸âš ï¸ Check our final slides [here](https://docs.google.com/presentation/d/17mf8qRFvCeGZUFsSpi91ZiALxGDk06jlI7QYSyJsF78/edit?slide=id.g31bab77bdae_0_0#slide=id.g31bab77bdae_0_0)!

ğŸ“ğŸ“ğŸ“ See the project paper [here](https://overleaf.cvc.uab.cat/read/tbtthcmqxmft#83be8e)!

Welcome to **Week 4** of our project, where we take **Image Captioning** a step further by leveraging **pre-trained multimodal models** and **parameter-efficient fine-tuning**. Building upon last weekâ€™s baseline and custom models, this week we explore:

- Finetuning a **pre-trained ViT-GPT2** model.
- Applying **PEFT (LoRA)** to adapt the model more efficiently.
- Evaluating zero-shot capabilities of models like **BLIP2**.

We continue to work on the [Food Ingredients and Recipes Dataset with Images](https://www.kaggle.com/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images), aiming to generate dish titles from food images.

---

### ğŸ½ï¸ Example:

<img src="https://assets.epicurious.com/photos/5f99a91e819b886aba0a2846/1:1/w_1920,c_limit/Chickensgiving_HERO_RECIPE_101920_1374_VOG_final.jpg" alt="Example Food Image" width="400"/>

**Desired Caption:** *"Miso-Butter Roast Chicken With Acorn Squash Panzanella"*

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ consts.py                 # Constants and configuration settings
â”œâ”€â”€ dataset.py                # Dataset class for main training/evaluation
â”œâ”€â”€ dataset_task_2.py         # Dataset class for direct evaluation (BLIP2)
â”œâ”€â”€ download_model.py         # Script to download pre-trained models
â”œâ”€â”€ evaluator.py              # Evaluation metrics and scoring
â”œâ”€â”€ plots.ipynb               # Notebook for plotting metrics/visualizations
â”œâ”€â”€ qualitative_eval.py       # Script for qualitative evaluation
â”œâ”€â”€ task1.py                  # Finetuning ViT-GPT2 model
â”œâ”€â”€ task2_PEFT.py             # LoRA-based PEFT finetuning
â”œâ”€â”€ task2_direct_eval.py      # BLIP2 direct evaluation (zero-shot)
â”œâ”€â”€ task2_test.py             # Script for testing/evaluation
â”œâ”€â”€ utils.py                  # Helper functions
```

---

## ğŸ‹ï¸â€â™‚ï¸ Tasks Overview

### ğŸ”§ Task 1: Finetuning ViT-GPT2

We fine-tune a **ViT-GPT2** model for image captioning using our food dataset. The model leverages powerful pre-trained vision and language components.

To run finetuning:

```bash
python task1.py
```

---

### ğŸ” Task 2.1: Direct Evaluation with BLIP2

We evaluate the zero-shot captioning ability of **BLIP2**, a strong multimodal model, without further training.

To run direct evaluation:

```bash
python task2_direct_eval.py
```

---

### ğŸª¶ Task 2.2: PEFT with LoRA

We apply **Low-Rank Adaptation (LoRA)** to fine-tune the model efficiently with fewer trainable parameters.

To run LoRA fine-tuning:

```bash
python task2_PEFT.py
```

---

## ğŸ–¼ï¸ Dataset

We use the same dataset as Week 3: **Food Ingredients and Recipes Dataset with Images**. Make sure it's downloaded and properly linked.

ğŸ”— **Dataset link:** [Kaggle Dataset](https://www.kaggle.com/datasets/pes12017000148/food-ingredients-and-recipe-dataset-with-images)

---

## ğŸ“Š Evaluation & Visualization

- Use `evaluator.py` to compute metrics like BLEU, METEOR, CIDEr, etc.
- Run `qualitative_eval.py` for visual comparison of predictions.
- Open `plots.ipynb` to explore and visualize training progress and results.

---

## ğŸš€ Setup Instructions

Make sure all required libraries are installed:

```bash
pip install -r requirements.txt
```

Then youâ€™re ready to run training, evaluation, and testing scripts!

---

## ğŸ“Œ Notes

- Modify paths and configurations as needed in `consts.py`.
- Check `download_model.py` to fetch fine tuned models from Wandb.
- LoRA-based fine-tuning is more memory-efficient â€” ideal for large-scale experiments.
- Zero-shot performance (BLIP2) offers a strong baseline for comparison.

---

For any questions or debugging help, reach out to the team! ğŸš€
